{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "One needs a way to choose between models: different model types, tuning parameters, and features. In general  to assess the fit of the model one can use summary measures of goodness of fit (such as $R^2$) or by assessing the predictive ability of the model (using  k-fold cross-validation).\n",
    "\n",
    "- Use a **goodness of fit** metric to assess how closely the model fits the data.\n",
    "\n",
    "- Use a **model evaluation procedure** to estimate the predictive ability. That is, how well a model will generalize to out-of-sample data.\n",
    "\n",
    "### Goodness of fit metric\n",
    "\n",
    "The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Goodness of fit metrics vary in their quality and reliability.\n",
    "\n",
    "Sometimes goodness of fit metrics are best fit metrics. For example, the likelihood ratio test statistic is a measure of the goodness of fit of a model, judged by whether an expanded form of the model provides a substantially improved fit.\n",
    "\n",
    "\n",
    "\n",
    "### Model evaluation procedures\n",
    "\n",
    "\n",
    "**Train/test split**\n",
    "  - Split the dataset into two pieces (usually of different sizes), so that the model can be trained and tested on different data.\n",
    "\n",
    "**K-fold cross-validation**\n",
    "  - Systematically create \"K\" train/test splits and average the results together.\n",
    "\n",
    "K-fold cross-validation is a simple, intuitive way to estimate prediction error.  K-fold cross-validation, which partitions the data into $k$ equally sized segments (called ‘folds’). One fold is held out for validation while the other k-1 folds are used to train the model and then used to predict the target variable in our testing data. This process is repeated k times, with the performance of each model in predicting the hold-out set being tracked using a performance metric such as accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall, Precision and Accuracy\n",
    "\n",
    "\n",
    "#### Classification outcomes\n",
    "\n",
    "* _true positives_\n",
    "* _true negatives_\n",
    "* _false positives_ (type I errors)\n",
    "* _false negatives_ (type II errors)\n",
    "\n",
    "Precision is number of true positive over all that are predicted to be positive.\n",
    "\n",
    "$$\\text{Precision}=\\frac{tp}{tp+fp} \\, $$\n",
    "\n",
    "Recall is number of true positive over all that are called positive.\n",
    "\n",
    "$$\\text{Recall}=\\frac{tp}{tp+fn} \\, $$\n",
    "\n",
    "Recall in this context is also referred to as the true positive rate or sensitivity, and precision is also referred to a positive predictive value]] (PPV); other related measures used in classification include true negative rate and accuracy. True negative rate is also called specificity.\n",
    "\n",
    "$$\\text{True negative rate}=\\frac{tn}{tn+fp} \\, $$\n",
    "\n",
    "$$\\text{Accuracy}=\\frac{tp+tn}{tp+tn+fp+fn} \\, $$\n",
    "\n",
    "See [What's the difference between accuracy and precision? - Matt Anticole](https://youtu.be/hRAFPdDppzs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity\n",
    "\n",
    "#### Sensitivity  \n",
    "\n",
    "_Sensitivity_ (also called the _true positive rate_, the _ precision_ and the _recall _, or _probability of detection_ in some fields) measures the proportion of positives that are correctly identified as such (i.e. the percentage of sick people who are correctly identified as having the condition).\n",
    "\n",
    "$$\\text{Sensitivity}=\\frac{tp}{tp+fn} \\, $$\n",
    "\n",
    "Note that is _sensitivity_ is also\n",
    "\n",
    "$$\\text{Recall}=\\frac{tp}{tp+fn} \\, $$\n",
    "\n",
    "Note that _sensitivity_ or _recall_ are all of the positives (actual and predicted). \n",
    "\n",
    "Assume you had a poison mushroom classifier, then you would want the _sensitivity_ to be very high. You would want to catch almost all instances of poison mushrooms. \n",
    "\n",
    "#### Specificity  \n",
    "\n",
    "_Specificity_ (also called the _true negative rate_) measures the proportion of negatives that are correctly identified as such (i.e., the percentage of healthy people who are correctly identified as not having the condition).\n",
    "\n",
    "$$\\text{Specificity}=\\frac{tn}{tn+fp} \\, $$\n",
    "\n",
    "Note that is _specificity_ is also\n",
    "\n",
    "$$\\text{True negative rate}=\\frac{tn}{tn+fp} \\, $$\n",
    "\n",
    "Note that _specificity_ or _true negative rate_ are all of the negatives (actual and predicted).\n",
    "\n",
    "Assume you had a poison mushroom classifier, then you would care less if the _specificity_ was high. If you falsely labled an edible poison mushroom as a poison mushroom, it would be much less of an issue than if you falsely labled a poison mushroom as an edible mushroom. \n",
    "\n",
    "These concepts often are expressed using a number of terms. Given\n",
    "\n",
    "* (number of) positive samples (P)\n",
    "* (number of) negative samples (N)\n",
    "* (number of) true positive (TP)\n",
    " eqv. with hit\n",
    "* (number of) true negative (TN)\n",
    " eqv. with correct rejection\n",
    "* (number of) false positive (FP)\n",
    " eqv. with false alarm , Type I error \n",
    "* (number of) false negative (FN)\n",
    " eqv. with miss, Type II error \n",
    "\n",
    "\n",
    "Then\n",
    "\n",
    "* sensitivity or true positive rate (TPR) or hit rate or recall \n",
    " \n",
    " $$\\mathit{TPR} = \\mathit{TP} / P = \\mathit{TP} / (\\mathit{TP}+\\mathit{FN})$$  \n",
    " \n",
    "* Specificity or true negative rate\n",
    "\n",
    " $$\\mathit{SPC} = \\mathit{TN} / N = \\mathit{TN} / (\\mathit{TN}+\\mathit{FP}) $$\n",
    " \n",
    " \n",
    "* precision or positive predictive value (PPV)  \n",
    "\n",
    " $$\\mathit{PPV} = \\mathit{TP} / (\\mathit{TP} + \\mathit{FP})$$\n",
    " \n",
    " \n",
    "* negative predictive value (NPV)\n",
    "\n",
    " $$\\mathit{NPV} = \\mathit{TN} / (\\mathit{TN} + \\mathit{FN})$$\n",
    " \n",
    " \n",
    "* fall-out or false positive rate (FPR)\n",
    "\n",
    " $$\\mathit{FPR} = \\mathit{FP} / N = \\mathit{FP} / (\\mathit{FP} + \\mathit{TN}) = 1-\\mathit{SPC}$$\n",
    " \n",
    " \n",
    "* false negative rate (FNR)\n",
    "\n",
    " $$\\mathit{FNR} = \\mathit{FN} / (\\mathit{TP} + \\mathit{FN}) = 1-\\mathit{TPR}$$\n",
    " \n",
    " \n",
    "* false discovery rate (FDR)\n",
    "\n",
    " $$\\mathit{FDR} = \\mathit{FP} / (\\mathit{TP} + \\mathit{FP}) = 1 - \\mathit{PPV} $$\n",
    " \n",
    "\n",
    "* accuracy (ACC)\n",
    "\n",
    " $$\\mathit{ACC} = (\\mathit{TP} + \\mathit{TN}) / (\\mathit{TP} + \\mathit{FP} + \\mathit{FN} + \\mathit{TN})$$\n",
    " \n",
    "* F1 score \n",
    "\n",
    " $$\\mathit{F1} = 2 \\mathit{TP} / (2 \\mathit{TP} + \\mathit{FP} + \\mathit{FN})$$\n",
    "\n",
    "* Matthews correlation coefficient (MCC)\n",
    "\n",
    " $$ \\frac{ \\mathit{TP} \\times \\mathit{TN} - \\mathit{FP} \\times \\mathit{FN} } {\\sqrt{ (\\mathit{TP}+\\mathit{FP}) ( \\mathit{TP} + \\mathit{FN} ) ( \\mathit{TN} + \\mathit{FP} ) ( \\mathit{TN} + \\mathit{FN} ) } }\n",
    "$$\n",
    "\n",
    "* Youden's J statistic  \n",
    "\n",
    " $$\\mathit{TPR} + \\mathit{SPC} - 1$$\n",
    "* Markedness \n",
    " $$\\mathit{PPV} + \\mathit{NPV} - 1$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity Tradeoff\n",
    "\n",
    "The sensitivity and specificity are dependent on the threshold used in a classifier.  sensitivity and specificity tradeoff.\n",
    "\n",
    "[Sensitivity and specificity tradeoff - Model Building and Validation]( https://youtu.be/5XMVhOZ5KMg)\n",
    "\n",
    "[The tradeoff between sensitivity and specificity](https://youtu.be/vtYDyGGeQyo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "In predictive analytics, a _table of confusion_ or  _confusion matrix_, is a table with two rows and two columns that reports the number of _false positives_, _false negatives_, _true positives_, and _true negatives_. This allows more detailed analysis than mere proportion of correct guesses (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the number of samples in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, the classifier could easily be biased into classifying all the samples as cats. The overall accuracy would be 95%, but in practice the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.\n",
    "\n",
    "| Confusion Matrix    | Actual positives                 | Actual negatives                |\n",
    "|---------------------|----------------------------------|---------------------------------|\n",
    "| Predicted positives | True positives                   | False positives (Type I errors) |\n",
    "| Predicted negatives | False negatives (Type II errors) | True negatives                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC Curves\n",
    "\n",
    "In statistics, a _receiver operating characteristic curve_, or _ROC curve_, is a  graph of a function|graphical plot  that illustrates the performance of a  binary classifier  system as its discrimination threshold is varied. \n",
    "\n",
    "The ROC curve is created by plotting the  true positive rate  (TPR) against the  false positive rate  (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, or 'probability of detection'\n",
    "\n",
    " \n",
    "$$\\text{True positive rate}=\\frac{tp}{tp+fn} \\, $$ \n",
    "\n",
    " \n",
    "The false-positive rate is also known as the fall-out  or 'probability of false alarm' and can be calculated as (1 − specificity ). \n",
    "\n",
    "$$\\text{False positive rate}=\\frac{fp}{tn+fp} \\, $$\n",
    "\n",
    "  \n",
    "$$\\text{Specificity}=\\frac{tn}{tn+fp} \\, $$\n",
    "\n",
    "Note that is _specificity_ is also\n",
    "\n",
    "$$\\text{True negative rate}=\\frac{tn}{tn+fp} \\, $$\n",
    "\n",
    "![ROC Curves](images/ROC_curves.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
