{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class 7- solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, metrics\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create sample data and fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# range(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x': range(100), 'y': range(100)})\n",
    "biased_df  = df.copy()\n",
    "biased_df.loc[:20, 'x'] = 1\n",
    "biased_df.loc[:20, 'y'] = 1\n",
    "\n",
    "def append_jitter(series):\n",
    "    jitter = np.random.random_sample(size=100)\n",
    "    return series + jitter\n",
    "\n",
    "df['x'] = append_jitter(df.x)\n",
    "df['y'] = append_jitter(df.y)\n",
    "\n",
    "biased_df['x'] = append_jitter(biased_df.x)\n",
    "biased_df['y'] = append_jitter(biased_df.y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x11d318110>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11d3182d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIgpgEZGIKIBFRCKiABYRiYgCWEQkIv8fovMTk/l/7TAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e04e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(x=\"x\", y=\"y\", data=biased_df)\n",
    "sns.lmplot(x=\"x\", y=\"y\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.142442848805\n"
     ]
    }
   ],
   "source": [
    "## fit\n",
    "lm = linear_model.LinearRegression().fit(df[['x']], df['y'])\n",
    "print metrics.mean_squared_error(df['y'], lm.predict(df[['x']]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156261014366\n"
     ]
    }
   ],
   "source": [
    "## biased fit\n",
    "lm = linear_model.LinearRegression().fit(biased_df[['x']], biased_df['y'])\n",
    "print metrics.mean_squared_error(df['y'], lm.predict(df[['x']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "#### Intro to cross validation with bike share data from last time. We will be modeling casual ridership. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2879</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
       "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
       "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
       "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
       "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
       "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
       "\n",
       "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
       "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
       "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
       "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
       "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
       "4           1  0.24  0.2879  0.75        0.0       0           1    1  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "wd = '../../assets/dataset/'\n",
    "bikeshare = pd.read_csv(wd + 'bikeshare.csv')\n",
    "bikeshare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>hr</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17379.0000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "      <td>17379.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8690.0000</td>\n",
       "      <td>2.501640</td>\n",
       "      <td>0.502561</td>\n",
       "      <td>6.537775</td>\n",
       "      <td>11.546752</td>\n",
       "      <td>0.028770</td>\n",
       "      <td>3.003683</td>\n",
       "      <td>0.682721</td>\n",
       "      <td>1.425283</td>\n",
       "      <td>0.496987</td>\n",
       "      <td>0.475775</td>\n",
       "      <td>0.627229</td>\n",
       "      <td>0.190098</td>\n",
       "      <td>35.676218</td>\n",
       "      <td>153.786869</td>\n",
       "      <td>189.463088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5017.0295</td>\n",
       "      <td>1.106918</td>\n",
       "      <td>0.500008</td>\n",
       "      <td>3.438776</td>\n",
       "      <td>6.914405</td>\n",
       "      <td>0.167165</td>\n",
       "      <td>2.005771</td>\n",
       "      <td>0.465431</td>\n",
       "      <td>0.639357</td>\n",
       "      <td>0.192556</td>\n",
       "      <td>0.171850</td>\n",
       "      <td>0.192930</td>\n",
       "      <td>0.122340</td>\n",
       "      <td>49.305030</td>\n",
       "      <td>151.357286</td>\n",
       "      <td>181.387599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4345.5000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.333300</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.104500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8690.0000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>13034.5000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>281.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17379.0000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.850700</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>977.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          instant        season            yr          mnth            hr  \\\n",
       "count  17379.0000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
       "mean    8690.0000      2.501640      0.502561      6.537775     11.546752   \n",
       "std     5017.0295      1.106918      0.500008      3.438776      6.914405   \n",
       "min        1.0000      1.000000      0.000000      1.000000      0.000000   \n",
       "25%     4345.5000      2.000000      0.000000      4.000000      6.000000   \n",
       "50%     8690.0000      3.000000      1.000000      7.000000     12.000000   \n",
       "75%    13034.5000      3.000000      1.000000     10.000000     18.000000   \n",
       "max    17379.0000      4.000000      1.000000     12.000000     23.000000   \n",
       "\n",
       "            holiday       weekday    workingday    weathersit          temp  \\\n",
       "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
       "mean       0.028770      3.003683      0.682721      1.425283      0.496987   \n",
       "std        0.167165      2.005771      0.465431      0.639357      0.192556   \n",
       "min        0.000000      0.000000      0.000000      1.000000      0.020000   \n",
       "25%        0.000000      1.000000      0.000000      1.000000      0.340000   \n",
       "50%        0.000000      3.000000      1.000000      1.000000      0.500000   \n",
       "75%        0.000000      5.000000      1.000000      2.000000      0.660000   \n",
       "max        1.000000      6.000000      1.000000      4.000000      1.000000   \n",
       "\n",
       "              atemp           hum     windspeed        casual    registered  \\\n",
       "count  17379.000000  17379.000000  17379.000000  17379.000000  17379.000000   \n",
       "mean       0.475775      0.627229      0.190098     35.676218    153.786869   \n",
       "std        0.171850      0.192930      0.122340     49.305030    151.357286   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.333300      0.480000      0.104500      4.000000     34.000000   \n",
       "50%        0.484800      0.630000      0.194000     17.000000    115.000000   \n",
       "75%        0.621200      0.780000      0.253700     48.000000    220.000000   \n",
       "max        1.000000      1.000000      0.850700    367.000000    886.000000   \n",
       "\n",
       "                cnt  \n",
       "count  17379.000000  \n",
       "mean     189.463088  \n",
       "std      181.387599  \n",
       "min        1.000000  \n",
       "25%       40.000000  \n",
       "50%      142.000000  \n",
       "75%      281.000000  \n",
       "max      977.000000  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bikeshare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Create dummy variables and set outcome (dependent) variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>hum</th>\n",
       "      <th>weather_1</th>\n",
       "      <th>weather_2</th>\n",
       "      <th>weather_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.24</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   temp   hum  weather_1  weather_2  weather_3\n",
       "0  0.24  0.81          1          0          0\n",
       "1  0.22  0.80          1          0          0\n",
       "2  0.22  0.80          1          0          0\n",
       "3  0.24  0.75          1          0          0\n",
       "4  0.24  0.75          1          0          0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather = pd.get_dummies(bikeshare.weathersit, prefix='weather')\n",
    "modeldata = bikeshare[['temp', 'hum']].join(weather[['weather_1', 'weather_2', 'weather_3']])\n",
    "y = bikeshare.casual \n",
    "modeldata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross valiation with 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.KFold"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(kf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "[ 0  2  3  4  6  7  8  9 10 11 12 13 16 19 20 22 23 25 27 28 29 30 31 32 33\n",
      " 34 36 37 41 43 44 45 47]\n",
      "[  1   5  14  15  17  18  21  24  26  35  38  39  40  42  46  48  49  50\n",
      "  55  60  66  70  83 100 117 123 125 132 137 141 155 159 160]\n",
      "<type 'numpy.ndarray'>\n",
      "[ 1  2  4  5  6  8  9 12 13 14 15 16 17 18 20 21 22 23 24 25 26 27 28 30 31\n",
      " 32 34 35 36 37 38 39 40]\n",
      "[  0   3   7  10  11  19  29  33  54  56  61  75  77  78  91  92  94  95\n",
      "  97 101 105 113 119 122 133 136 143 147 154 156 174 177 180]\n",
      "<type 'numpy.ndarray'>\n",
      "[ 0  1  2  3  5  6  7 10 11 14 15 17 18 19 21 22 23 24 25 26 27 28 29 31 33\n",
      " 34 35 36 38 39 40 41 42]\n",
      "[  4   8   9  12  13  16  20  30  32  37  53  68  69  71  74  76  81  93\n",
      " 104 108 116 120 126 129 131 135 140 142 149 151 152 162 175]\n",
      "<type 'numpy.ndarray'>\n",
      "[ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 24 26 28\n",
      " 29 30 31 32 33 35 37 38]\n",
      "[  2  23  25  27  34  36  41  43  44  47  52  57  58  64  65  67  73  82\n",
      "  85  86  88  90  96  98  99 102 103 106 107 111 112 114 115]\n",
      "<type 'numpy.ndarray'>\n",
      "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 23 24 25 26\n",
      " 27 29 30 32 33 34 35 36]\n",
      "[  6  22  28  31  45  51  59  62  63  72  79  80  84  87  89 109 110 118\n",
      " 130 134 138 145 150 157 158 164 171 173 176 185 186 187 189]\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in kf:\n",
    "    print type(train_index)    \n",
    "    print train_index[0:33]  \n",
    "    print test_index[0:33]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ CROSS VALIDATION each fold ~~~~\n",
      "Model 1\n",
      "MSE: 1785.43966147\n",
      "R2: 0.311875748762\n",
      "Model 2\n",
      "MSE: 1555.75999804\n",
      "R2: 0.311887385414\n",
      "Model 3\n",
      "MSE: 1759.78016928\n",
      "R2: 0.31191055443\n",
      "Model 4\n",
      "MSE: 1686.31666308\n",
      "R2: 0.311889076303\n",
      "Model 5\n",
      "MSE: 1579.67863831\n",
      "R2: 0.311922859046\n",
      "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
      "Mean of MSE for all folds: 1673.39502604\n",
      "Mean of R2 for all folds: 0.311897124791\n",
      "STD of MSE for all folds: 92.5247244226\n",
      "STD of R2 for all folds: 1.70831806843e-05\n"
     ]
    }
   ],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n= 0\n",
    "print \"~~~~ CROSS VALIDATION each fold ~~~~\"\n",
    "for train_index, test_index in kf:\n",
    "    lm = linear_model.LinearRegression().fit(modeldata.iloc[train_index], y.iloc[train_index])\n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lm.predict(modeldata.iloc[test_index])))\n",
    "    scores.append(lm.score(modeldata, y))\n",
    "    n+=1\n",
    "    print 'Model', n\n",
    "    print 'MSE:', mse_values[n-1]\n",
    "    print 'R2:', scores[n-1]\n",
    "\n",
    "\n",
    "print \"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\"\n",
    "print 'Mean of MSE for all folds:', np.mean(mse_values)\n",
    "print 'Mean of R2 for all folds:', np.mean(scores)\n",
    "print 'STD of MSE for all folds:', np.std(mse_values)\n",
    "print 'STD of R2 for all folds:', np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~ Single Model ~~~~\n",
      "MSE of single model: 1672.58110765\n",
      "R2:  0.311934605989\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "print \"~~~~ Single Model ~~~~\"\n",
    "print 'MSE of single model:', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'R2: ', lm.score(modeldata, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check\n",
    "While the cross validated approach here generated more overall error, which of the two approaches would predict new data more accurately: the single model or the cross validated, averaged one? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: this score will be lower with the single model in the case, but we're trading off bias error for generalized error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Advanced: There are ways to improve our model with regularization. \n",
    "Let's check out the effects on MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ OLS ~~~\n",
      "OLS MSE:  1672.58110765\n",
      "OLS R2: 0.311934605989\n",
      "~~~ Lasso ~~~\n",
      "Lasso MSE:  1725.41581608\n",
      "Lasso R2: 0.290199495922\n",
      "~~~ Ridge ~~~\n",
      "Ridge MSE:  1672.60490113\n",
      "Ridge R2: 0.311924817843\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.LinearRegression().fit(modeldata, y)\n",
    "print \"~~~ OLS ~~~\"\n",
    "print 'OLS MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'OLS R2:', lm.score(modeldata, y)\n",
    "\n",
    "lm = linear_model.Lasso().fit(modeldata, y)\n",
    "print \"~~~ Lasso ~~~\"\n",
    "print 'Lasso MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'Lasso R2:', lm.score(modeldata, y)\n",
    "\n",
    "lm = linear_model.Ridge().fit(modeldata, y)\n",
    "print \"~~~ Ridge ~~~\"\n",
    "print 'Ridge MSE: ', metrics.mean_squared_error(y, lm.predict(modeldata))\n",
    "print 'Ridge R2:', lm.score(modeldata, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out the alphas can be done by \"hand\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1e-10\n",
      "[ 112.68901765  -84.01121684  -24.68489063  -21.00314493  -21.71893628]\n",
      "1672.58110765\n",
      "Alpha: 1e-09\n",
      "[ 112.68901765  -84.01121684  -24.68489061  -21.00314491  -21.71893626]\n",
      "1672.58110765\n",
      "Alpha: 1e-08\n",
      "[ 112.68901765  -84.01121684  -24.6848904   -21.00314471  -21.71893606]\n",
      "1672.58110765\n",
      "Alpha: 1e-07\n",
      "[ 112.68901763  -84.01121682  -24.68488837  -21.00314268  -21.71893403]\n",
      "1672.58110765\n",
      "Alpha: 1e-06\n",
      "[ 112.68901745  -84.01121667  -24.68486804  -21.00312237  -21.71891373]\n",
      "1672.58110765\n",
      "Alpha: 1e-05\n",
      "[ 112.68901562  -84.01121509  -24.68466472  -21.00291929  -21.71871079]\n",
      "1672.58110765\n",
      "Alpha: 0.0001\n",
      "[ 112.68899732  -84.01119938  -24.68263174  -21.00088873  -21.71668162]\n",
      "1672.58110765\n",
      "Alpha: 0.001\n",
      "[ 112.68881437  -84.01104228  -24.66232204  -20.98060316  -21.69640993]\n",
      "1672.58110774\n",
      "Alpha: 0.01\n",
      "[ 112.68698753  -84.00947323  -24.46121539  -20.77973778  -21.49568404]\n",
      "1672.58111645\n",
      "Alpha: 0.1\n",
      "[ 112.66896732  -83.99396383  -22.63109556  -18.95202277  -19.66942371]\n",
      "1672.58185208\n",
      "Alpha: 1.0\n",
      "[ 112.50129738  -83.84805622  -13.38214934   -9.72671278  -10.46162477]\n",
      "1672.60490113\n",
      "Alpha: 10.0\n",
      "[ 110.96062533  -82.49604961   -3.94431741   -0.51765034   -1.45024412]\n",
      "1672.83347262\n",
      "Alpha: 100.0\n",
      "[ 97.69060562 -71.17602377  -0.31585194   1.18284675  -1.33281591]\n",
      "1686.31830362\n",
      "Alpha: 1000.0\n",
      "[ 44.59923075 -30.85843772   5.07876321   0.05369643  -5.107457  ]\n",
      "1937.81576044\n",
      "Alpha: 10000.0\n",
      "[ 7.03007064 -5.07733082  3.29039029 -1.2136063  -2.06842808]\n",
      "2314.83675678\n",
      "Alpha: 100000.0\n",
      "[ 0.75195708 -0.56490872  0.52067881 -0.25075496 -0.26895254]\n",
      "2415.77806566\n",
      "Alpha: 1000000.0\n",
      "[ 0.07576571 -0.05727511  0.05520142 -0.0273591  -0.02774349]\n",
      "2429.28026459\n",
      "Alpha: 10000000.0\n",
      "[ 0.00758239 -0.00573569  0.0055535  -0.00276043 -0.00278317]\n",
      "2430.68891798\n",
      "Alpha: 100000000.0\n",
      "[ 0.0007583  -0.00057365  0.00055569 -0.00027629 -0.00027841]\n",
      "2430.83041212\n",
      "Alpha: 1000000000.0\n",
      "[  7.58303020e-05  -5.73659720e-05   5.55719458e-05  -2.76314619e-05\n",
      "  -2.78414555e-05]\n",
      "2430.84456787\n",
      "Alpha: 10000000000.0\n",
      "[  7.58303603e-06  -5.73660542e-06   5.55722818e-06  -2.76317091e-06\n",
      "  -2.78415441e-06]\n",
      "2430.84598351\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-10, 10, 21)\n",
    "for a in alphas:\n",
    "    print 'Alpha:', a\n",
    "    lm = linear_model.Ridge(alpha=a)\n",
    "    lm.fit(modeldata, y)\n",
    "    print lm.coef_\n",
    "    print metrics.mean_squared_error(y, lm.predict(modeldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or we can use grid search to make this faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': array([  1.00000e-10,   1.00000e-09,   1.00000e-08,   1.00000e-07,\n",
       "         1.00000e-06,   1.00000e-05,   1.00000e-04,   1.00000e-03,\n",
       "         1.00000e-02,   1.00000e-01,   1.00000e+00,   1.00000e+01,\n",
       "         1.00000e+02,   1.00000e+03,   1.00000e+04,   1.00000e+05,\n",
       "         1.00000e+06,   1.00000e+07,   1.00000e+08,   1.00000e+09,\n",
       "         1.00000e+10])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='mean_squared_error',\n",
       "       verbose=0)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import grid_search\n",
    "\n",
    "alphas = np.logspace(-10, 10, 21)\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.Ridge(),\n",
    "    param_grid={'alpha': alphas},\n",
    "    scoring='mean_squared_error')\n",
    "\n",
    "gs.fit(modeldata, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1814.09369133\n"
     ]
    }
   ],
   "source": [
    "print gs.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### mean squared error here comes in negative, so let's make it positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1814.09369133\n"
     ]
    }
   ],
   "source": [
    "print -gs.best_score_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### explains which grid_search setup worked best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge(alpha=10.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=False, random_state=None, solver='auto', tol=0.001)\n"
     ]
    }
   ],
   "source": [
    "print gs.best_estimator_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### shows all the grid pairings and their performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: -1817.58711, std: 542.14315, params: {'alpha': 1e-10}, mean: -1817.58711, std: 542.14315, params: {'alpha': 1.0000000000000001e-09}, mean: -1817.58711, std: 542.14315, params: {'alpha': 1e-08}, mean: -1817.58711, std: 542.14315, params: {'alpha': 9.9999999999999995e-08}, mean: -1817.58711, std: 542.14315, params: {'alpha': 9.9999999999999995e-07}, mean: -1817.58711, std: 542.14317, params: {'alpha': 1.0000000000000001e-05}, mean: -1817.58707, std: 542.14331, params: {'alpha': 0.0001}, mean: -1817.58663, std: 542.14477, params: {'alpha': 0.001}, mean: -1817.58230, std: 542.15933, params: {'alpha': 0.01}, mean: -1817.54318, std: 542.30102, params: {'alpha': 0.10000000000000001}, mean: -1817.20111, std: 543.63587, params: {'alpha': 1.0}, mean: -1814.09369, std: 556.35563, params: {'alpha': 10.0}, mean: -1818.51694, std: 653.68607, params: {'alpha': 100.0}, mean: -2125.58777, std: 872.45270, params: {'alpha': 1000.0}, mean: -2458.08836, std: 951.30428, params: {'alpha': 10000.0}, mean: -2532.21151, std: 962.80083, params: {'alpha': 100000.0}, mean: -2541.38479, std: 963.98339, params: {'alpha': 1000000.0}, mean: -2542.32833, std: 964.10141, params: {'alpha': 10000000.0}, mean: -2542.42296, std: 964.11321, params: {'alpha': 100000000.0}, mean: -2542.43242, std: 964.11439, params: {'alpha': 1000000000.0}, mean: -2542.43337, std: 964.11450, params: {'alpha': 10000000000.0}]\n"
     ]
    }
   ],
   "source": [
    "print gs.grid_scores_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 is better than 6.2\n",
      "found better solution! using 5.2\n",
      "4.2 is better than 5.2\n",
      "found better solution! using 4.2\n",
      "3.2 is better than 4.2\n",
      "found better solution! using 3.2\n",
      "2.2 is better than 3.2\n",
      "found better solution! using 2.2\n",
      "1.2 is better than 2.2\n",
      "found better solution! using 1.2\n",
      "0.2 is better than 1.2\n",
      "found better solution! using 0.2\n",
      "6.0 is closest to 6.2\n"
     ]
    }
   ],
   "source": [
    "num_to_approach, start, steps, optimized = 6.2, 0., [-1, 1], False\n",
    "while not optimized:\n",
    "    current_distance = num_to_approach - start\n",
    "    got_better = False\n",
    "    next_steps = [start + i for i in steps]\n",
    "    for n in next_steps:\n",
    "        distance = np.abs(num_to_approach - n)\n",
    "        if distance < current_distance:\n",
    "            got_better = True\n",
    "            print distance, 'is better than', current_distance\n",
    "            current_distance = distance\n",
    "            start = n\n",
    "    if got_better:\n",
    "        print 'found better solution! using', current_distance\n",
    "        a += 1\n",
    "    else:\n",
    "        optimized = True\n",
    "        print start, 'is closest to', num_to_approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "For the DP example below, it might be a great idea for students to take the code and implement a stopping point, similar to what n_iter would do in gradient descent.\n",
    "\n",
    "There can be a great conversation about stopping early and still _kinda_ getting the right result vs taking a longer time to solve and having a more precise model.\n",
    "\n",
    "That solution is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2 is better than 6.2\n",
      "found better solution! using 5.2\n",
      "4.2 is better than 5.2\n",
      "found better solution! using 4.2\n",
      "3.2 is better than 4.2\n",
      "found better solution! using 3.2\n",
      "2.2 is better than 3.2\n",
      "found better solution! using 2.2\n",
      "stopping iterations\n"
     ]
    }
   ],
   "source": [
    "num_to_approach, start, steps, optimized = 6.2, 0., [-1, 1], False\n",
    "n_iter = 0\n",
    "while not optimized:\n",
    "    if n_iter > 3:\n",
    "        print 'stopping iterations'\n",
    "        break\n",
    "    n_iter += 1\n",
    "    current_distance = num_to_approach - start\n",
    "    got_better = False\n",
    "    next_steps = [start + i for i in steps]\n",
    "    for n in next_steps:\n",
    "        distance = np.abs(num_to_approach - n)\n",
    "        if distance < current_distance:\n",
    "            got_better = True\n",
    "            print distance, 'is better than', current_distance\n",
    "            current_distance = distance\n",
    "            start = n\n",
    "    if got_better:\n",
    "        print 'found better solution! using', current_distance\n",
    "        a += 1\n",
    "    else:\n",
    "        optimized = True\n",
    "        print start, 'is closest to', num_to_approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Demo: Application of Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent R2: 0.308291561077\n",
      "Gradient Descent MSE: 1681.43678932\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.SGDRegressor()\n",
    "lm.fit(modeldata, y)\n",
    "print \"Gradient Descent R2:\", lm.score(modeldata, y)\n",
    "print \"Gradient Descent MSE:\", metrics.mean_squared_error(y, lm.predict(modeldata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Check: Untuned, how well did gradient descent perform compared to OLS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous Result (from above):\n",
    "~~~~ SUMMARY OF CROSS VALIDATION ~~~~\n",
    "Mean of MSE for all folds: 1780.97924083\n",
    "Mean of R2 for all folds: 0.306643649561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: similar R2, MSE is lower for GR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Independent Practice: Bike data revisited\n",
    "\n",
    "There are tons of ways to approach a regression problem. The regularization techniques appended to ordinary least squares optimizes the size of coefficients to best account for error. Gradient Descent also introduces learning rate (how aggressively do we solve the problem), epsilon (at what point do we say the error margin is acceptable), and iterations (when should we stop no matter what?)\n",
    "\n",
    "For this deliverable, our goals are to:\n",
    "\n",
    "- implement the gradient descent approach to our bike-share modeling problem,\n",
    "- show how gradient descent solves and optimizes the solution,\n",
    "- demonstrate the grid_search module!\n",
    "\n",
    "While exploring the Gradient Descent regressor object, you'll build a grid search using the stochastic gradient descent estimator for the bike-share data set. Continue with either the model you evaluated last class or the simpler one from today. In particular, be sure to implement the \"param_grid\" in the grid search to get answers for the following questions:\n",
    "\n",
    "- With a set of alpha values between 10^-10 and 10^-1, how does the mean squared error change?\n",
    "- Based on the data, we know when to properly use l1 vs l2 regularization. By using a grid search with l1_ratios between 0 and 1 (increasing every 0.05), does that statement hold true? If not, did gradient descent have enough iterations?\n",
    "- How do these results change when you alter the learning rate (eta0)?\n",
    "\n",
    "**Bonus**: Can you see the advantages and disadvantages of using gradient descent after finishing this exercise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starter Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST ESTIMATOR\n",
      "1689.02636451\n",
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
      "       random_state=None, shuffle=True, verbose=0, warm_start=False)\n",
      "ALL ESTIMATORS\n",
      "[mean: -1689.02636, std: 94.15777, params: {}]\n"
     ]
    }
   ],
   "source": [
    "params = {} # put your gradient descent parameters here\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "print 'BEST ESTIMATOR'\n",
    "print -gs.best_score_\n",
    "print gs.best_estimator_\n",
    "print 'ALL ESTIMATORS'\n",
    "print gs.grid_scores_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent Practice Solution\n",
    "\n",
    "This code shows the variety of challenges and some student gotchas. The plots will help showcase what should be learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. With a set of alpha values between 10^-10 and 10^-1, how does the mean squared error change?\n",
    "2. We know when to properly use l1 vs l2 regularization based on the data. By using a grid search with l1_ratios between 0 and 1 (increasing every 0.05), does that statement hold true?\n",
    "    * (if it didn't look like it, did gradient descent have enough iterations?)\n",
    "3. How do results change when you alter the learning rate (power_t)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e-10   1.00000000e-09   1.00000000e-08   1.00000000e-07\n",
      "   1.00000000e-06   1.00000000e-05   1.00000000e-04   1.00000000e-03\n",
      "   1.00000000e-02   1.00000000e-01]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-54b558490483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'alpha'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bear/anaconda/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bear/anaconda/envs/py27/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bear/anaconda/envs/py27/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bear/anaconda/envs/py27/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bear/anaconda/envs/py27/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/src/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-10, -1, 10)\n",
    "print alphas\n",
    "params = {'alpha':alphas, } # put your gradient descent parameters here\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "grid = pd.DataFrame(gs.grid_scores_)\n",
    "grid[0] = grid[0].apply(lambda x: x['alpha'])\n",
    "grid[1] = grid[1].apply(lambda x: -x)\n",
    "grid.columns = ['alpha', 'mean_squared_error', 'cv']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the alphas available, it looks like at mean squared error stays generally flat with incredibly small alpha values, but starting at $10^{-3}$, the error begins to elbow. We probably don't have much of a different in performance with other alpha values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.plot('alpha', 'mean_squared_error', logx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At alpha values of either .1 or 1, the l1_ratio works best closer to 1! Interesting. At other values of alpha they should see similar results, though the graphs aren't as clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_2_ratios = [float(i) / 100 for i in range(0, 101, 5)]\n",
    "print l1_2_ratios\n",
    "params = {'l1_ratio':l1_2_ratios, 'penalty': ['elasticnet'], 'alpha': [.1], 'n_iter': [50]}\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "grid = pd.DataFrame(gs.grid_scores_)\n",
    "grid[0] = grid[0].apply(lambda x: x['l1_ratio'])\n",
    "grid[1] = grid[1].apply(lambda x: -x)\n",
    "grid.columns = ['l1_ratio', 'mean_squared_error', 'cv']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.plot('l1_ratio', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning = range(1, 50)\n",
    "print learning\n",
    "params = {'eta0':learning, 'n_iter': [50]}\n",
    "gs = grid_search.GridSearchCV(\n",
    "    estimator=linear_model.SGDRegressor(),\n",
    "    cv=cross_validation.KFold(len(modeldata), n_folds=5, shuffle=True),\n",
    "    param_grid=params,\n",
    "    scoring='mean_squared_error',\n",
    "    )\n",
    "\n",
    "gs.fit(modeldata, y)\n",
    "\n",
    "grid = pd.DataFrame(gs.grid_scores_)\n",
    "grid[0] = grid[0].apply(lambda x: x['eta0'])\n",
    "grid[1] = grid[1].apply(lambda x: -x)\n",
    "grid.columns = ['eta0', 'mean_squared_error', 'cv']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it should be apparent that as the initial learning rate increases, the error should _also_ increase. And what happens when the initial learning rate is too high? A dramatic increase in error. Students should recognize the importance of learning rate and what values it should be set at, the smaller generally the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid.plot('eta0', 'mean_squared_error', logy=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
